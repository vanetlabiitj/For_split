{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494411a9-3556-4e8e-92aa-46b99316fcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jayant\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e8887a-032b-4f3c-887e-2e053a2b635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99e5c26-b407-486f-a8a0-9b1ff13b2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_images.reshape((60000, 784))\n",
    "x_test = test_images.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff552f38-e11b-46ef-b0fa-9e601cb3547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test  = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e2f334-5def-4920-afc6-7b884384f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.keras.utils.to_categorical(train_labels, dtype =\"uint8\")\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, dtype =\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dd088e-767a-49c2-bb24-824b643403f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9abd7e-d7b1-446f-b332-db2f2cd51e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85723df8-c3fa-4ce0-84cb-15436662c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c0aa0c-6101-4e2a-9dbd-46e619bdcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"Computes softmax function.\n",
    "\n",
    "    z: array of input values.\n",
    "\n",
    "    Returns an array of outputs with the same shape as z.\"\"\"\n",
    "    # For numerical stability: make the maximum of z's to be 0.\n",
    "    cache = Z\n",
    "    shiftz = Z - np.max(Z)\n",
    "    exps = np.exp(shiftz)\n",
    "    A = exps / np.sum(exps)\n",
    "    return A,cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb6b244-0d1c-4ff9-ad7d-39e0858892e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    #print(\"Shapes\",Z.shape, A.shape, W.shape, b.shape )\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bde851-52d8-47b7-8d8c-570257b9a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "   \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c81526d-5a1f-4d5b-9b8f-30709fd363cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_CE(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, (number of classes,batch_size)\n",
    "    Y -- true \"label\" vector (number of classes, batch_size)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(\"shape of y_hat\",AL.shape )\n",
    "#     print(\"shape of y\",Y.shape )\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * np.sum((-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T)))\n",
    "#     print(\"Cost shape: \", cost.shape)\n",
    "\n",
    "    \n",
    "    cost = np.squeeze(cost)# To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "#     print(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def compute_cost_categorical_CE(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, (number of classes,batch_size)\n",
    "    Y -- true \"label\" vector (number of classes, batch_size)\n",
    "\n",
    "    Returns:\n",
    "    cost -- Categorical_cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "#     print(\"shape of y_hat\",AL.shape )\n",
    "#     print(\"shape of y\",Y.shape )\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "    # Compute loss from aL and y.\n",
    "    cost = - (1./m) * np.sum(np.multiply(Y,np.log(AL)))\n",
    "#     print(\"Cost shape: \", cost.shape)\n",
    "\n",
    "    \n",
    "    cost = np.squeeze(cost)# To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "#     print(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa105d98-3535-45d0-94ed-ef629b21c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax_backward wont handle the batch as of now\n",
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"Computes the gradient of the softmax function.\n",
    "\n",
    "    z: (T, 1) array of input values where the gradient is computed. T is the\n",
    "       number of output classes.\n",
    "\n",
    "    Returns D (T, T) the Jacobian matrix of softmax(z) at the given z. D[i, j]\n",
    "    is DjSi - the partial derivative of Si w.r.t. input j.\n",
    "    \"\"\"\n",
    "    z = cache\n",
    "    Sz, _ = softmax(z)\n",
    "#     print(\"DA Shape \", dA.shape)\n",
    "#     print(\"Sz Shape \", Sz.shape)\n",
    "    \n",
    "    # -SjSi can be computed using an outer product between Sz and itself. Then\n",
    "    # we add back Si for the i=j cases by adding a diagonal matrix with the\n",
    "    # values of Si on its diagonal.\n",
    "   \n",
    "    D = -np.outer(Sz, Sz) + np.diag(Sz.flatten())\n",
    "    return D.dot(dA)\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    \n",
    "    print(dA.shape,Z.shape)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d8abfa6-1400-475d-b9ba-3f503c171b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "        \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76670f66-4703-4da0-8792-b3df2ce7e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (10,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "439d759b-b160-40d7-9321-762e4f6df426",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= x_train.T\n",
    "Y=train_labels.T\n",
    "learning_rate = 0.0075\n",
    "num_iterations = 100\n",
    "print_cost=False\n",
    "\n",
    "cost = 0\n",
    "\n",
    "np.random.seed(1)\n",
    "grads = {}\n",
    "costs = []                              # to keep track of the cost\n",
    "m = X.shape[1]                           # number of examples\n",
    "(n_x, n_h, n_y) = (784,20,10)\n",
    "costs_batch = []                             # to keep track of the cost\n",
    "costs_iterations = []\n",
    "grads['dW1'] = 0\n",
    "grads['db1'] = 0\n",
    "grads['dW2'] = 0\n",
    "grads['db2'] = 0\n",
    "iteration = 0\n",
    "\n",
    "# Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "CACHE1 = []\n",
    "CACHE2 = []\n",
    "A1STORE = []\n",
    "A2STORE = []\n",
    "dA1STORE = []\n",
    "dA2STORE = []\n",
    "# Loop (gradient descent)\n",
    "\n",
    "temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c15080-3d9a-4f4b-8359-56366cb317a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, cache = linear_activation_forward(np.reshape(X[:,0], (784,1)), W1, b1, activation=\"relu\")\n",
    "layer_data = { 'A' : A,\n",
    "               'cache' : cache }\n",
    "device_message = {\"Source\":\"edge_1\",\"target\":\"edge_2\",\"type\":\"forward\",\"Data\": layer_data }\n",
    "msg = pickle.dumps(device_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52efca25-5b66-4530-a68c-512791491517",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"10.6.1.155\" #ip address\n",
    "device_id = 9002 #port Number\n",
    "def send_data(device_id,data):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.connect((HOST, device_id))\n",
    "        s.sendall(b\"Data resive\")\n",
    "        \n",
    "def recive_data(device_id):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((HOST, device_id))\n",
    "        s.listen()\n",
    "        conn, addr = s.accept()\n",
    "        data = b''\n",
    "        with conn:\n",
    "            while True:\n",
    "                data_recv = conn.recv(4096)\n",
    "                data += data_recv\n",
    "                if not data_recv:\n",
    "                    break\n",
    "        msg = pickle.loads(data)\n",
    "            \n",
    "    print(f'recived data at {device_id} \\n')\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df87c54e-95f4-4726-98d5-8c1348027324",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'socket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     send_data(send_to_devices,msg)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[43mrecive_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     pA \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m     pCache \u001b[38;5;241m=\u001b[39m msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m, in \u001b[0;36mrecive_data\u001b[1;34m(device_id)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecive_data\u001b[39m(device_id):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241m.\u001b[39msocket(socket\u001b[38;5;241m.\u001b[39mAF_INET, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m     10\u001b[0m         s\u001b[38;5;241m.\u001b[39mbind((HOST, device_id))\n\u001b[0;32m     11\u001b[0m         s\u001b[38;5;241m.\u001b[39mlisten()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'socket' is not defined"
     ]
    }
   ],
   "source": [
    "first_devices = False \n",
    "send_to_devices = 9003\n",
    "if __name__ ==\"__main__\":\n",
    "    if first_devices:\n",
    "        time.sleep(2)\n",
    "        send_data(send_to_devices,msg)\n",
    "    else:\n",
    "        msg = recive_data(device_id)\n",
    "        pA = msg[\"Data\"]['A']\n",
    "        pCache = msg[\"Data\"]['cache']\n",
    "        A, cache = linear_activation_forward(pA, W2, b2, activation=\"softmax\")\n",
    "        layer_data = { 'A' : A,\n",
    "                       'cache' : cache }\n",
    "        device_message = {\"Source\":\"edge_2\",\"target\":\"edge_3\",\"type\":\"forward\",\"Data\": layer_data }\n",
    "        msg = pickle.dumps(device_message)\n",
    "        send_data(send_to_devices,msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8806311-bd3d-47d3-934b-3a62c4258fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
